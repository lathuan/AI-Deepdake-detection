# predict.py
import os
import argparse
import cv2
import matplotlib.pyplot as plt
from utils.model_loader import load_trained_model
from utils.face_detector import FaceDetector
from utils.video_processor import VideoProcessor

def display_result(video_path, result):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n"""
    color = 'red' if result['prediction'] == 'FAKE' else 'green'
    emoji = '‚ùå' if result['prediction'] == 'FAKE' else '‚úÖ'
    
    print(f"\n{'='*60}")
    print(f"{emoji} K·∫æT QU·∫¢ D·ª∞ ƒêO√ÅN DEEPFAKE {emoji}")
    print(f"{'='*60}")
    print(f"üìπ Video: {os.path.basename(video_path)}")
    print(f"üîç K·∫øt qu·∫£: {result['prediction']}")
    print(f"üìä ƒê·ªô tin c·∫≠y: {result['confidence']:.4f}")
    print(f"üéØ X√°c su·∫•t FAKE: {result['probability']:.4f}")
    print(f"üë§ S·ªë khu√¥n m·∫∑t ph√¢n t√≠ch: {result['num_faces']}")
    
    # Hi·ªÉn th·ªã khu√¥n m·∫∑t n·∫øu c√≥
    if 'faces_sample' in result and len(result['faces_sample']) > 0:
        print(f"\nüñºÔ∏è  M·∫´u khu√¥n m·∫∑t tr√≠ch xu·∫•t:")
        
        fig, axes = plt.subplots(1, len(result['faces_sample']), figsize=(12, 3))
        if len(result['faces_sample']) == 1:
            axes = [axes]
            
        for i, (face, ax) in enumerate(zip(result['faces_sample'], axes)):
            face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
            ax.imshow(face_rgb)
            ax.set_title(f'Frame {i+1}')
            ax.axis('off')
        
        plt.tight_layout()
        plt.show()

def main():
    parser = argparse.ArgumentParser(description='Deepfake Detection')
    parser.add_argument('--video', type=str, required=True, help='ƒê∆∞·ªùng d·∫´n video c·∫ßn ki·ªÉm tra')
    parser.add_argument('--model', type=str, default='model/best_deepfake_model_dfd.pth', help='ƒê∆∞·ªùng d·∫´n model')
    parser.add_argument('--device', type=str, default='auto', help='Device: auto, cuda, ho·∫∑c cpu')
    
    args = parser.parse_args()
    
    # Ki·ªÉm tra file video
    if not os.path.exists(args.video):
        print(f"‚ùå Video kh√¥ng t·ªìn t·∫°i: {args.video}")
        return
    
    # Load model
    try:
        model, device = load_trained_model(args.model, args.device)
    except Exception as e:
        print(f"‚ùå L·ªói khi load model: {e}")
        return
    
    # Kh·ªüi t·∫°o face detector v√† video processor
    face_detector = FaceDetector(model_path='yolov8l-face-lindevs.pt')
    video_processor = VideoProcessor(face_detector)
    
    # D·ª± ƒëo√°n
    result = video_processor.predict_video(args.video, model, device)
    
    # Hi·ªÉn th·ªã k·∫øt qu·∫£
    display_result(args.video, result)

if __name__ == "__main__":
    main()