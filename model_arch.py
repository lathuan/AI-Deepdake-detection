# model_arch.py - PHI√äN B·∫¢N C·∫¢I THI·ªÜN (TH√äM DROPOUT & L·ªöPM·ªöI - S·ª¨A L·ªñI isinstance)

import tensorflow as tf
from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Concatenate, BatchNormalization, Dropout
from tensorflow.keras.applications import Xception, ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import Precision, Recall, AUC
import os
import shutil


# --- H√ÄM T·∫†O M√î H√åNH TWO-STREAM (C·∫¢I THI·ªÜN) ---
def create_two_stream_model(face_input_shape, context_input_shape, dropout_stream=0.4, 
                           dropout_combined=0.3, dense_1=128, dense_2=64, dense_3=32):
    """
    T·∫°o m√¥ h√¨nh two-stream v·ªõi:
    - Dropout ƒë·ªÉ tr√°nh overfitting
    - L·ªõp Dense trung gian th√™m
    - Metrics: Precision, Recall, AUC
    
    Args:
        face_input_shape: Tuple (height, width, channels) cho Face stream
        context_input_shape: Tuple (height, width, channels) cho Context stream
        dropout_stream: Dropout rate sau Face/Context output
        dropout_combined: Dropout rate sau combined layers
        dense_1: S·ªë unit cho dense layer 1 (Face & Context output)
        dense_2: S·ªë unit cho dense layer 2 (combined)
        dense_3: S·ªë unit cho dense layer 3 (combined)
    
    Returns:
        Keras Model
    """
    
    # ===== NH√ÅNH 1: KHU√îN M·∫∂T (FACE STREAM) - XCEPTION =====
    face_input = Input(shape=face_input_shape, name='face_input')
    
    # T·∫£i Xception pretrained
    face_base = Xception(weights='imagenet', include_top=False, 
                        input_tensor=face_input, name='xception')
    
    # Kh√≥a to√†n b·ªô l·ªõp ban ƒë·∫ßu
    for layer in face_base.layers:
        layer.trainable = False
    
    # X·ª≠ l√Ω output
    x = face_base.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(dense_1, activation='relu', name='face_dense_1')(x)
    x = Dropout(dropout_stream, name='face_dropout_1')(x)  # TH√äM DROPOUT
    face_output = x
    
    
    # ===== NH√ÅNH 2: NG·ªÆ C·∫¢NH (CONTEXT STREAM) - RESNET50 =====
    context_input = Input(shape=context_input_shape, name='context_input')
    
    # T·∫£i ResNet50 pretrained
    context_base = ResNet50(weights='imagenet', include_top=False, 
                           input_tensor=context_input, name='resnet50')
    
    # Kh√≥a to√†n b·ªô l·ªõp ban ƒë·∫ßu
    for layer in context_base.layers:
        layer.trainable = False
    
    # X·ª≠ l√Ω output
    y = context_base.output
    y = GlobalAveragePooling2D()(y)
    y = Dense(dense_1, activation='relu', name='context_dense_1')(y)
    y = Dropout(dropout_stream, name='context_dropout_1')(y)  # TH√äM DROPOUT
    context_output = y
    
    
    # ===== K·∫æT H·ª¢P V√Ä PH√ÇN LO·∫†I =====
    combined = Concatenate(name='concatenate')([face_output, context_output])
    combined = BatchNormalization(name='batch_norm_1')(combined)
    
    # Dense layer 1
    combined = Dense(dense_2, activation='relu', name='combined_dense_1')(combined)
    combined = Dropout(dropout_combined, name='combined_dropout_1')(combined)  # TH√äM DROPOUT
    
    # Dense layer 2 (L·ªöP M·ªöI TH√äM V√ÄO)
    combined = Dense(dense_3, activation='relu', name='combined_dense_2')(combined)
    combined = Dropout(dropout_combined * 0.67, name='combined_dropout_2')(combined)  # DROPOUT CHA H∆†N
    
    # Output layer
    output = Dense(2, activation='softmax', name='output')(combined)
    
    # T·∫°o model
    model = Model(inputs=[face_input, context_input], outputs=output, 
                 name='TwoStreamDeepfakeDetector')
    
    return model


# --- H√ÄM COMPILE M√î H√åNH V·ªöI METRICS C·∫¢I THI·ªÜN ---
def compile_model(model, learning_rate, use_focal_loss=False):
    """
    Compile m√¥ h√¨nh v·ªõi loss, optimizer v√† metrics c·∫£i thi·ªán
    
    Args:
        model: Keras model
        learning_rate: Learning rate cho optimizer
        use_focal_loss: S·ª≠ d·ª•ng Focal Loss (t·ªët h∆°n cho imbalanced data)
                       N·∫øu False, d√πng categorical crossentropy
    
    Returns:
        Compiled model
    """
    
    if use_focal_loss:
        try:
            import tensorflow_addons as tfa
            loss_fn = tfa.losses.SigmoidFocalCrossEntropy()
            print("‚úì D√πng Focal Loss cho imbalanced data")
        except ImportError:
            print("‚ö† tensorflow_addons kh√¥ng ƒë∆∞·ª£c c√†i. S·ª≠ d·ª•ng Categorical Crossentropy")
            loss_fn = tf.keras.losses.CategoricalCrossentropy()
    else:
        loss_fn = tf.keras.losses.CategoricalCrossentropy()
    
    model.compile(
        optimizer=Adam(learning_rate=learning_rate),
        loss=loss_fn,
        metrics=[
            'accuracy',
            Precision(name='precision'),  # TH√äM PRECISION
            Recall(name='recall'),         # TH√äM RECALL
            AUC(name='auc')               # TH√äM AUC
        ]
    )
    
    return model


# --- H√ÄM TINH CH·ªàNH (FINE-TUNING) - S·ª¨A B·∫∞NG PATTERN MATCHING =====
def fine_tune_two_stream_model(model, learning_rate_finetune, 
                              unfreeze_xception=50, unfreeze_resnet=50):
    """
    Fine-tune m√¥ h√¨nh b·∫±ng c√°ch m·ªü kh√≥a l·ªõp cu·ªëi d·ª±a tr√™n pattern
    
    Args:
        model: Trained model
        learning_rate_finetune: Learning rate cho fine-tuning
        unfreeze_xception: S·ªë l·ªõp cu·ªëi c·ªßa Xception ƒë·ªÉ m·ªü kh√≥a
        unfreeze_resnet: S·ªë l·ªõp cu·ªëi c·ªßa ResNet50 ƒë·ªÉ m·ªü kh√≥a
    
    Returns:
        Model sau khi fine-tune
    """
    
    print("\nüîì B·∫ÆT ƒê·∫¶U FINE-TUNING...")
    
    # Ph√¢n lo·∫°i layers theo pattern
    # ResNet50: layers b·∫Øt ƒë·∫ßu v·ªõi conv2, conv3, conv4, conv5
    # Xception: layers b·∫Øt ƒë·∫ßu v·ªõi block1, block2, ..., block14
    
    resnet_layers = []
    xception_layers = []
    other_layers = []
    
    for layer in model.layers:
        layer_name = layer.name.lower()
        # ResNet50 pattern
        if any(pattern in layer_name for pattern in ['conv2_', 'conv3_', 'conv4_', 'conv5_']):
            resnet_layers.append(layer)
        # Xception pattern
        elif any(pattern in layer_name for pattern in ['block1_', 'block2_', 'block3_', 'block4_', 
                                                        'block5_', 'block6_', 'block7_', 'block8_', 
                                                        'block9_', 'block10_', 'block11_', 'block12_', 
                                                        'block13_', 'block14_']):
            xception_layers.append(layer)
        else:
            other_layers.append(layer)
    
    print(f"\nüìä Ph√¢n lo·∫°i layers:")
    print(f"   ResNet50 layers: {len(resnet_layers)}")
    print(f"   Xception layers: {len(xception_layers)}")
    print(f"   Other layers: {len(other_layers)}")
    
    # M·ªû KH√ìA C√ÅC L·ªöP CU·ªêI
    count_xception = 0
    count_resnet = 0
    
    # M·ªü kh√≥a Xception
    print(f"\nüîì M·ªü kh√≥a {unfreeze_xception} l·ªõp cu·ªëi c·ªßa Xception...")
    for layer in xception_layers[-unfreeze_xception:]:
        if not isinstance(layer, BatchNormalization):
            layer.trainable = True
            count_xception += 1
    print(f"   ‚úì ƒê√£ m·ªü kh√≥a {count_xception}/{len(xception_layers)} l·ªõp Xception")
    
    # M·ªü kh√≥a ResNet50
    print(f"üîì M·ªü kh√≥a {unfreeze_resnet} l·ªõp cu·ªëi c·ªßa ResNet50...")
    for layer in resnet_layers[-unfreeze_resnet:]:
        if not isinstance(layer, BatchNormalization):
            layer.trainable = True
            count_resnet += 1
    print(f"   ‚úì ƒê√£ m·ªü kh√≥a {count_resnet}/{len(resnet_layers)} l·ªõp ResNet50")
    
    print(f"\n‚úì T·ªïng c·ªông ƒë√£ m·ªü kh√≥a {count_xception + count_resnet} l·ªõp n·ªÅn\n")
    
    # COMPILE L·∫†I V·ªöI LEARNING RATE M·ªöI
    model = compile_model(model, learning_rate_finetune, use_focal_loss=False)
    
    return model


# --- H√ÄM IN C·∫§U TR√öC M√î H√åNH (DEBUG) ---
def print_model_summary(model, verbose=False):
    """
    In th√¥ng tin chi ti·∫øt v·ªÅ m√¥ h√¨nh
    """
    import os
    import shutil

    # L·∫•y k√≠ch th∆∞·ªõc terminal hi·ªán t·∫°i
    terminal_width = shutil.get_terminal_size().columns

    # N·∫øu terminal qu√° nh·ªè, ƒëi·ªÅu ch·ªânh ƒë·ªô r·ªông
    if terminal_width < 120:
        print(f"‚ö†Ô∏è  Terminal qu√° nh·ªè ({terminal_width} c·ªôt), ƒëi·ªÅu ch·ªânh ƒë·ªô r·ªông...")
        os.environ['COLUMNS'] = '120'  # Ch·ªânh console r·ªông h∆°n

    print("\n" + "="*80)
    print("üìä TH√îNG TIN M√î H√åNH")
    print("="*80)

    # In summary tr√™n terminal v·ªõi ƒë·ªô r·ªông m·ªõi
    model.summary()

    if verbose:
        print("\nüìã CHI TI·∫æT C√ÅC L·ªöP:")
        for i, layer in enumerate(model.layers):
            trainable = "üîì" if layer.trainable else "üîí"
            params = layer.count_params()
            print(f"  {i:2d}. {trainable} {layer.name:30s} | {layer.__class__.__name__:20s} | {params:>12,} params")

    print("="*80 + "\n")