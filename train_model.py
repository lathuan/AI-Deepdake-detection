
# train_model.py - PHI√äN B·∫¢N C·∫¢I TI·∫æN V3 (FIX class_weight error)

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, 
                                       ModelCheckpoint, TensorBoard)
from sklearn.utils.class_weight import compute_class_weight

from config import *
from model_arch import (create_two_stream_model, fine_tune_two_stream_model, 
                       compile_model, print_model_summary)


def get_two_stream_generator(data_dir, target_size_face, target_size_context, 
                            batch_size, subset, validation_split):
    """
    T·∫°o data generator cho hai nh√°nh v·ªõi augmentation n√¢ng cao
    ‚úì FIX: Generator format ch√≠nh x√°c
    """
    
    # Augmentation c·∫•u h√¨nh
    datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=15,
        width_shift_range=0.15,
        height_shift_range=0.15,
        shear_range=0.15,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest',
        validation_split=validation_split
    )
    
    face_gen = datagen.flow_from_directory(
        data_dir,
        target_size=target_size_face,
        batch_size=batch_size,
        class_mode='categorical',
        subset=subset,
        seed=42,
        interpolation='bilinear'
    )
    
    context_gen = datagen.flow_from_directory(
        data_dir,
        target_size=target_size_context,
        batch_size=batch_size,
        class_mode='categorical',
        subset=subset,
        seed=42,
        interpolation='bilinear'
    )
    
    total_samples = face_gen.n
    
    def two_stream_generator():
        while True:
            X_face = face_gen.__next__()
            X_context = context_gen.__next__()
            # ‚úì FIX: Yield ƒë√∫ng format (dict inputs, labels)
            yield (
                {'face_input': X_face[0], 'context_input': X_context[0]}, 
                X_face[1]
            )
    
    return two_stream_generator(), total_samples


def train_model(use_class_weights=True, use_focal_loss=False):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh v·ªõi hai giai ƒëo·∫°n: Warm-up v√† Fine-tuning
    ‚úì FIX: B·ªé class_weight v√¨ generator kh√¥ng h·ªó tr·ª£
    """
    
    # ===== B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU =====
    print("\n" + "="*80)
    print("üìÇ CHU·∫®N B·ªä D·ªÆ LI·ªÜU")
    print("="*80)
    
    train_gen, train_samples = get_two_stream_generator(
        data_dir=DATA_DIR,
        target_size_face=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT),
        target_size_context=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        subset='training',
        validation_split=VALIDATION_SPLIT
    )
    
    val_gen, val_samples = get_two_stream_generator(
        data_dir=DATA_DIR,
        target_size_face=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT),
        target_size_context=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        subset='validation',
        validation_split=VALIDATION_SPLIT
    )
    
    train_steps = max(1, train_samples // BATCH_SIZE)
    val_steps = max(1, val_samples // BATCH_SIZE)
    
    print(f"‚úì Training samples: {train_samples} ({train_steps} steps)")
    print(f"‚úì Validation samples: {val_samples} ({val_steps} steps)")
    print(f"‚úì Batch size: {BATCH_SIZE}")
    
    if train_steps == 0:
        print("‚ùå L·ªñI: train_steps = 0. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu v√† BATCH_SIZE")
        return
    
    
    # ===== B∆Ø·ªöC 2: T·∫†O M√î H√åNH =====
    print("\n" + "="*80)
    print("üèóÔ∏è  T·∫†O M√î H√åNH TWO-STREAM")
    print("="*80)
    
    model = create_two_stream_model(
        face_input_shape=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT, 3),
        context_input_shape=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT, 3),
        dropout_stream=DROPOUT_RATE_STREAM,
        dropout_combined=DROPOUT_RATE_COMBINED,
        dense_1=DENSE_UNITS_1,
        dense_2=DENSE_UNITS_2,
        dense_3=DENSE_UNITS_3,
        l2_reg=L2_REGULARIZATION
    )
    
    print_model_summary(model, verbose=False)
    
    model = compile_model(model, LEARNING_RATE_WARMUP, use_focal_loss=use_focal_loss)
    
    
    # ===== B∆Ø·ªöC 3: H·ªéI T·ª§ (WARMUP) =====
    print("\n" + "="*80)
    print("üî• GIAI ƒêO·∫†N 1: WARM-UP (L·ªõp n·ªÅn b·ªã ƒë√≥ng bƒÉng)")
    print("="*80)
    print(f"Learning Rate: {LEARNING_RATE_WARMUP}")
    print(f"Epochs: {EPOCHS_WARMUP}")
    
    warmup_callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=PATIENCE_WARMUP,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=LR_REDUCE_FACTOR,
            patience=LR_REDUCE_PATIENCE,
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=os.path.join(MODEL_OUTPUT_DIR, 'logs_warmup'),
            histogram_freq=1
        )
    ]
    
    # ‚úì FIX: B·ªé class_weight v√¨ generator kh√¥ng h·ªó tr·ª£
    print("\nüìä Ghi ch√∫: class_weight kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£ v·ªõi custom generator")
    print("   M√¥ h√¨nh s·∫Ω t·ª± c√¢n b·∫±ng qua augmentation v√† dropout")
    
    history_warmup = model.fit(
        train_gen,
        steps_per_epoch=train_steps,
        epochs=EPOCHS_WARMUP,
        validation_data=val_gen,
        validation_steps=val_steps,
        callbacks=warmup_callbacks,
        verbose=1
    )
    
    print("\n‚úì Ho√†n th√†nh giai ƒëo·∫°n Warm-up")
    
    
    # ===== B∆Ø·ªöC 4: TINH CH·ªàNH (FINE-TUNING) =====
    print("\n" + "="*80)
    print("üîì GIAI ƒêO·∫†N 2: FINE-TUNING (M·ªü kh√≥a l·ªõp cu·ªëi)")
    print("="*80)
    print(f"Learning Rate: {LEARNING_RATE_FINETUNE}")
    print(f"Epochs: {EPOCHS_FINETUNE}")
    
    model = fine_tune_two_stream_model(
        model,
        LEARNING_RATE_FINETUNE,
        unfreeze_xception=UNFREEZE_LAYERS_XCEPTION,
        unfreeze_resnet=UNFREEZE_LAYERS_RESNET
    )
    
    finetune_callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=PATIENCE_FINETUNE,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=LR_REDUCE_FACTOR,
            patience=LR_REDUCE_PATIENCE,
            min_lr=1e-7,
            verbose=1
        ),
        ModelCheckpoint(
            filepath=os.path.join(MODEL_OUTPUT_DIR, MODEL_NAME),
            monitor='val_loss',
            save_best_only=True,
            verbose=1,
            mode='min'
        ),
        TensorBoard(
            log_dir=os.path.join(MODEL_OUTPUT_DIR, 'logs_finetune'),
            histogram_freq=1
        )
    ]
    
    # ‚úì FIX: B·ªé class_weight
    history_finetune = model.fit(
        train_gen,
        steps_per_epoch=train_steps,
        epochs=EPOCHS_FINETUNE,
        validation_data=val_gen,
        validation_steps=val_steps,
        callbacks=finetune_callbacks,
        verbose=1
    )
    
    print("\n‚úì Ho√†n th√†nh giai ƒëo·∫°n Fine-tuning")
    
    
    # ===== B∆Ø·ªöC 5: L∆ØU MODEL =====
    print("\n" + "="*80)
    print("üíæ K·∫æT QU·∫¢ H·ªÆU LUY·ªÜN")
    print("="*80)
    
    best_model_path = os.path.join(MODEL_OUTPUT_DIR, MODEL_NAME)
    
    print(f"\n‚úÖ Model t·ªët nh·∫•t ƒë∆∞·ª£c l∆∞u t·∫°i:")
    print(f"   üìÅ {best_model_path}")
    
    # Hi·ªÉn th·ªã th√¥ng tin model
    if len(history_warmup.history['val_loss']) > 0:
        print(f"\nüìä Th√¥ng tin training:")
        print(f"   ‚îú‚îÄ Warmup Val Loss (cu·ªëi): {history_warmup.history['val_loss'][-1]:.4f}")
        print(f"   ‚îú‚îÄ Warmup Val Accuracy: {history_warmup.history['val_accuracy'][-1]:.4f}")
        print(f"   ‚îú‚îÄ Fine-tune Val Loss (cu·ªëi): {history_finetune.history['val_loss'][-1]:.4f}")
        print(f"   ‚îî‚îÄ Fine-tune Val Accuracy: {history_finetune.history['val_accuracy'][-1]:.4f}")
    
    print("\n" + "="*80)
    print("üéâ HO√ÄN TH√ÄNH HU·∫§n LUY·ªÜN")
    print("="*80 + "\n")
    
    return model, history_warmup, history_finetune


if __name__ == '__main__':
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    if not os.path.exists(MODEL_OUTPUT_DIR):
        os.makedirs(MODEL_OUTPUT_DIR)
        print(f"‚úì T·∫°o th∆∞ m·ª•c: {MODEL_OUTPUT_DIR}")
    
    # B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
    print("\n" + "="*80)
    print("üöÄ B·∫ÆT ƒê·∫¶U HU·∫§n LUY·ªÜN DEEPFAKE DETECTION")
    print("="*80)
    
    model, hist_warmup, hist_finetune = train_model(
        use_class_weights=USE_CLASS_WEIGHTS, 
        use_focal_loss=USE_FOCAL_LOSS
    )
    
    print("\n‚úÖ TRAINING COMPLETED SUCCESSFULLY!")