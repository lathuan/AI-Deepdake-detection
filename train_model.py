# train_model.py - PHI√äN B·∫¢N C·∫¢I THI·ªÜN

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import (EarlyStopping, ReduceLROnPlateau, 
                                       ModelCheckpoint, TensorBoard)
from sklearn.utils.class_weight import compute_class_weight

# Import config v√† model
from config import *
from model_arch import (create_two_stream_model, fine_tune_two_stream_model, 
                       compile_model, print_model_summary)


# --- H√ÄM T·∫†O DATA GENERATOR CHO M√î H√åNH HAI NH√ÅNH (C·∫¢I THI·ªÜN) ---
def get_two_stream_generator(data_dir, target_size_face, target_size_context, 
                            batch_size, subset, validation_split):
    """
    T·∫°o data generator cho hai nh√°nh x·ª≠ l√Ω ·∫£nh k√≠ch th∆∞·ªõc kh√°c nhau
    
    Args:
        data_dir: ƒê∆∞·ªùng d·∫´n th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu
        target_size_face: K√≠ch th∆∞·ªõc ·∫£nh cho Face stream (H, W)
        target_size_context: K√≠ch th∆∞·ªõc ·∫£nh cho Context stream (H, W)
        batch_size: Batch size
        subset: 'training' ho·∫∑c 'validation'
        validation_split: T·ª∑ l·ªá validation split
    
    Returns:
        Generator v√† s·ªë l∆∞·ª£ng samples
    """
    
    datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=10,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.1,
        zoom_range=0.1,
        horizontal_flip=True,
        validation_split=validation_split
    )
    
    face_gen = datagen.flow_from_directory(
        data_dir,
        target_size=target_size_face,
        batch_size=batch_size,
        class_mode='categorical',
        subset=subset,
        seed=42
    )
    
    context_gen = datagen.flow_from_directory(
        data_dir,
        target_size=target_size_context,
        batch_size=batch_size,
        class_mode='categorical',
        subset=subset,
        seed=42
    )
    
    total_samples = face_gen.n
    
    def two_stream_generator():
        while True:
            X_face = face_gen.__next__()
            X_context = context_gen.__next__()
            yield ({'face_input': X_face[0], 'context_input': X_context[0]}, X_face[1])
    
    return two_stream_generator(), total_samples


# --- H√ÄM T√çNH CLASS WEIGHTS CHO IMBALANCED DATA (S·ª¨A - KH√îNG TRUY·ªÄN class_weight) ---
def calculate_class_weights(data_dir, subset='training', validation_split=0.2):
    """
    T√≠nh class weights ƒë·ªÉ x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng d·ªØ li·ªáu
    
    Returns:
        Dict: {class_index: weight}
    """
    datagen = ImageDataGenerator(validation_split=validation_split)
    
    gen = datagen.flow_from_directory(
        data_dir,
        target_size=(224, 224),
        batch_size=1,
        class_mode='categorical',
        subset=subset,
        shuffle=False
    )
    
    # L·∫•y labels c·ªßa t·∫•t c·∫£ samples
    all_labels = []
    for _ in range(gen.n):
        _, labels = gen.__next__()
        all_labels.append(np.argmax(labels, axis=1)[0])
    
    all_labels = np.array(all_labels)
    
    # T√≠nh class weights
    class_weights = compute_class_weight('balanced',
                                        classes=np.array([0, 1]),
                                        y=all_labels)
    
    class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
    
    print(f"üìä Class Weights (x·ª≠ l√Ω imbalanced data):")
    print(f"   Class 0 (Real): {class_weight_dict[0]:.4f}")
    print(f"   Class 1 (Deepfake): {class_weight_dict[1]:.4f}")
    
    return class_weight_dict


# --- H√ÄM CH√çNH ƒê·ªÇ HU·∫§N LUY·ªÜN ---
def train_model(use_class_weights=True, use_focal_loss=False):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh hai giai ƒëo·∫°n: Warm-up v√† Fine-tuning
    
    Args:
        use_class_weights: S·ª≠ d·ª•ng class weights cho imbalanced data
        use_focal_loss: S·ª≠ d·ª•ng Focal Loss (requires tensorflow_addons)
    """
    
    # ===== B∆Ø·ªöC 1: CHU·∫®N B·ªä D·ªÆ LI·ªÜU =====
    print("\n" + "="*80)
    print("üìÇ CHU·∫®N B·ªä D·ªÆ LI·ªÜU")
    print("="*80)
    
    train_gen, train_samples = get_two_stream_generator(
        data_dir=DATA_DIR,
        target_size_face=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT),
        target_size_context=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        subset='training',
        validation_split=VALIDATION_SPLIT
    )
    
    val_gen, val_samples = get_two_stream_generator(
        data_dir=DATA_DIR,
        target_size_face=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT),
        target_size_context=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT),
        batch_size=BATCH_SIZE,
        subset='validation',
        validation_split=VALIDATION_SPLIT
    )
    
    train_steps = train_samples // BATCH_SIZE
    val_steps = val_samples // BATCH_SIZE
    
    print(f"‚úì Training samples: {train_samples} ({train_steps} steps)")
    print(f"‚úì Validation samples: {val_samples} ({val_steps} steps)")
    
    if train_steps == 0:
        print("‚ùå L·ªñI: train_steps = 0. Ki·ªÉm tra l·∫°i d·ªØ li·ªáu v√† BATCH_SIZE")
        return
    
    # T√≠nh class weights n·∫øu c·∫ßn
    class_weight_dict = None
    if use_class_weights:
        print("\nüìä T√≠nh to√°n Class Weights...")
        class_weight_dict = calculate_class_weights(DATA_DIR, subset='training', 
                                                   validation_split=VALIDATION_SPLIT)
    
    
    # ===== B∆Ø·ªöC 2: T·∫†O M√î H√åNH =====
    print("\n" + "="*80)
    print("üèó T·∫†O M√î H√åNH TWO-STREAM")
    print("="*80)
    
    model = create_two_stream_model(
        face_input_shape=(FACE_IMG_WIDTH, FACE_IMG_HEIGHT, 3),
        context_input_shape=(CONTEXT_IMG_WIDTH, CONTEXT_IMG_HEIGHT, 3),
        dropout_stream=DROPOUT_RATE_STREAM,
        dropout_combined=DROPOUT_RATE_COMBINED,
        dense_1=DENSE_UNITS_1,
        dense_2=DENSE_UNITS_2,
        dense_3=DENSE_UNITS_3
    )
    
    # In th√¥ng tin m√¥ h√¨nh
    print_model_summary(model, verbose=False)
    
    # Compile m√¥ h√¨nh
    model = compile_model(model, LEARNING_RATE_WARMUP, use_focal_loss=use_focal_loss)
    
    
    # ===== B∆Ø·ªöC 3: H·ªòI T·ª§ (WARMUP) =====
    print("\n" + "="*80)
    print("üî• GIAI ƒêO·∫†N 1: WARM-UP (L·ªõp n·ªÅn b·ªã ƒë√≥ng bƒÉng)")
    print("="*80)
    print(f"Learning Rate: {LEARNING_RATE_WARMUP}")
    print(f"Epochs: {EPOCHS_WARMUP}")
    
    warmup_callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=PATIENCE_WARMUP,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=LR_REDUCE_FACTOR,
            patience=LR_REDUCE_PATIENCE,
            min_lr=1e-6,
            verbose=1
        ),
        TensorBoard(
            log_dir=os.path.join(MODEL_OUTPUT_DIR, 'logs_warmup'),
            histogram_freq=1
        )
    ]
    
    # S·ª¨A: Lo·∫°i b·ªè class_weight trong fit()
    history_warmup = model.fit(
        train_gen,
        steps_per_epoch=train_steps,
        epochs=EPOCHS_WARMUP,
        validation_data=val_gen,
        validation_steps=val_steps,
        callbacks=warmup_callbacks,
        verbose=1
        # KH√îNG TRUY·ªÄN class_weight ƒê√ÇY TRUY·ªÄN
    )
    
    print("\n‚úì Ho√†n th√†nh giai ƒëo·∫°n Warm-up")
    
    
    # ===== B∆Ø·ªöC 4: TINH CH·ªàNH (FINE-TUNING) =====
    print("\n" + "="*80)
    print("üîì GIAI ƒêO·∫†N 2: FINE-TUNING (M·ªü kh√≥a l·ªõp cu·ªëi)")
    print("="*80)
    print(f"Learning Rate: {LEARNING_RATE_FINETUNE}")
    print(f"Epochs: {EPOCHS_FINETUNE}")
    
    model = fine_tune_two_stream_model(
        model,
        LEARNING_RATE_FINETUNE,
        unfreeze_xception=UNFREEZE_LAYERS_XCEPTION,
        unfreeze_resnet=UNFREEZE_LAYERS_RESNET
    )
    
    finetune_callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=PATIENCE_FINETUNE,
            restore_best_weights=True,
            verbose=1
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=LR_REDUCE_FACTOR,
            patience=LR_REDUCE_PATIENCE,
            min_lr=1e-7,
            verbose=1
        ),
        ModelCheckpoint(
            filepath=os.path.join(MODEL_OUTPUT_DIR, MODEL_NAME),
            monitor='val_loss',
            save_best_only=True,
            verbose=1
        ),
        TensorBoard(
            log_dir=os.path.join(MODEL_OUTPUT_DIR, 'logs_finetune'),
            histogram_freq=1
        )
    ]
    
    # S·ª¨A: Lo·∫°i b·ªè class_weight trong fit()
    history_finetune = model.fit(
        train_gen,
        steps_per_epoch=train_steps,
        epochs=EPOCHS_FINETUNE,
        validation_data=val_gen,
        validation_steps=val_steps,
        callbacks=finetune_callbacks,
        verbose=1
        # KH√îNG TRUY·ªÄN class_weight ƒê√ÇY TRUY·ªÄN
    )
    
    print("\n‚úì Ho√†n th√†nh giai ƒëo·∫°n Fine-tuning")
    
    
    # ===== B∆Ø·ªöC 5: L∆ØU M√î H√åNH =====
    print("\n" + "="*80)
    print("üíæ L∆ØU M√î H√åNH")
    print("="*80)
    
    final_model_path = os.path.join(MODEL_OUTPUT_DIR, FINAL_MODEL_NAME)
    model.save(final_model_path)
    print(f"‚úì M√¥ h√¨nh cu·ªëi c√πng: {final_model_path}")
    
    best_model_path = os.path.join(MODEL_OUTPUT_DIR, MODEL_NAME)
    print(f"‚úì M√¥ h√¨nh t·ªët nh·∫•t: {best_model_path}")
    
    print("\n" + "="*80)
    print("üéâ HO√ÄN TH√ÄNH HU·∫§N LUY·ªÜN")
    print("="*80 + "\n")
    
    return model, history_warmup, history_finetune


if __name__ == '__main__':
    # T·∫°o th∆∞ m·ª•c output n·∫øu ch∆∞a c√≥
    if not os.path.exists(MODEL_OUTPUT_DIR):
        os.makedirs(MODEL_OUTPUT_DIR)
        print(f"‚úì T·∫°o th∆∞ m·ª•c: {MODEL_OUTPUT_DIR}")
    
    # B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán
    # use_focal_loss=True n·∫øu b·∫°n c√≥ tensorflow_addons c√†i
    model, hist_warmup, hist_finetune = train_model(use_class_weights=True, 
                                                    use_focal_loss=False)